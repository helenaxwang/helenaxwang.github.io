<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <title>Helena Wang | Github Pages</title>

    <link rel="stylesheet" href="stylesheets/styles.css">
    <link rel="stylesheet" href="stylesheets/pygment_trac.css">
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
  </head>
  <body>
    <div class="wrapper">
      <header>
        <div class="profile-img">
          <img src="images/profile.jpg">
        </div>
        <h1>Helena Wang</h1>
        <p class="view">
          <a class="soc-icon" href="https://github.com/helenaxwang"><img src="images/ico_github.png"></a>
          <a class="soc-icon" href="https://www.linkedin.com/pub/helena-wang/3/3/61a"><img src="images/ico_linkedin.png"></a>
          <a class="soc-icon" href="https://twitter.com/helenaXwang"><img src="images/ico_twitter.png"></a>
        </p>
        <p class="view">
          <a href="index.html">About me</a><br>
          <a href="research.html">Research</a><br>
          <a href="cv.html">CV</a>
        </p>

      </header>
      <section>
<h2>
<a name="research" class="anchor" href="#research"><span class="octicon octicon-link"></span></a>Research</h2>

<p>

Below is a top-level summary of some of the projects I have worked on, grouped more or less by topic. 

</p>

<h4>
<a name="motion" class="anchor" href="#motion"><span class="octicon octicon-link"></span></a>Encoding of visual motion in human visual cortex</h4>
<p>  
Multivariate pattern analysis (MPVA) is a commonly used analysis method to infer the perceptual and cognitive state of an observer from fMRI measurements. For example, in vision experiments, the response properties of a visual stimulus are read out, or “decoded”, from the spatially distributed pattern of voxel responses in the brain. High decoding accuracy in a given visual area is commonly interpreted to indicate the selectivity (at the level of individual or clusters of neurons) in that area for the decoded visual response property. Our results provide evidence against that notion. We demonstrate that, in case of visual motion, fMRI-based motion decoding has little or no dependence on the underlying functional organization of motion selectivity. This work also investigates the spatial pattern of neural representations that are necessary for successful fMRI-based decoding. <br><br>

<small>This work was presented at the <a href="http://www.abstractsonline.com/plan/start.aspx?mkey=%7B8D2A5BEC-4825-4CD6-9439-B42BB151D1CF%7D">2013 Society for Neuroscience conference</a> in San Diego. </small>
</p>
<br>

<h4>
<a name="mt" class="anchor" href="#mt"><span class="octicon octicon-link"></span></a>Encoding of visual motion in nonhuman primates</h4>
<p>  
Neurons in area MT of the macaque monkey encode visual motion. Some of these neurons encode the true motion directions of visual patterns, which is a non-trivial computation that requires the nonlinear integration of “component directions” corresponding to multiple local, oriented features within the pattern. In this work, I analyze a previously-studied population of ~800 MT cells to investigate how various response properties of these neurons contribute to their degree of pattern direction selectivity. These statistical relationships help build functional models underlying the mechanisms of motion integration by MT neurons. 
<br><br>
<small>This work was done in the laboratory of <a href="http://www.cns.nyu.edu/corefaculty/Movshon.php">Tony Movshon</a> and presented at the <a href="http://www.abstractsonline.com/plan/start.aspx?mkey={E5D5C83F-CE2D-4D71-9DD6-FC7231E090FB}">2010 Society for Neuroscience conference </a> in San Diego. </small>
</p>
<br>


<h4>
<a name="texture" class="anchor" href="#texture"><span class="octicon octicon-link"></span></a>Using perception of second-order visual stimuli to infer neural computation</h4>
<p>  
In pattern vision, “first-order” patterns are those with boundaries defined by changes in luminance (pixel intensity). The processing of first-order images is largely a linear computation known to take place in primary visual cortex (V1): the perceptual sensitivity to these images is linked systematically to the responses of V1 neurons. <a href="http://www.cns.nyu.edu/home/msl/#research">“Second-order”</a> patterns are those with boundaries defined by image statistics other than pixel intensity. The model commonly used to explain how neurons might compute, and how humans might perceive, this kind of boundaries involves a cascade of linear computations with intervening nonlinearities (e.g., filtering, rectification, normalization, filtering). It has also been proposed that each of these component computations can be thought of as a <a href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3273486/">“canonical” computation</a> that is ubiquitous in different areas of cortex and can be combined in various ways to perform different sensory and cognitive functions. In this study, we tested for the existence of and characterized the computation of normalization in second-order vision, by manipulating and measuring observers’ perceptual sensitivity to second-order visual stimuli. This allows us to extrapolate from the computational theories of first-order vision to make inferences about computations of higher-order visual processing and other areas of cortex beyond V1. 
<br><br>
<small>
This work was a collaboration with <a href = "http://www.cns.nyu.edu/home/msl/">Mike Landy</a>.<br> 
It was <a href="http://www.ncbi.nlm.nih.gov/pubmed/22811987">published</a> in Wang HX, Heeger DJ, Landy MS, Responses to second-order texture modulations undergo surround suppression, Vision Research, 60:192-200, 2012. <a href="http://www.cns.nyu.edu/heegerlab/content/publications/Wang-VisRes2012.pdf">reprint</a>.<br>
A preliminary version of this work was also presented at the <a href="http://www.journalofvision.org/content/11/11/1173.abstract">2011 Vision Sciences Society</a> conference. 
</small>
</p>
<br>



<h4>
<a name="eyemovements" class="anchor" href="#eyemovements"><span class="octicon octicon-link"></span></a>Temporal reliability of exploratory eye movements</h4>
<p>  
How people move their eyes to complex visual stimuli likely involves a variety of cognitive factors. But surprisingly, to a certain class of dynamic, engaging stimuli, eye movements are highly reliable across observers and across repetitions. We asked whether that reliability of eye movements depends on the accumulation of visual information over time. So we systematically manipulated the temporal context of movie clips (by scrambling the temporal order), measured people’s eye movements, and developed a mathematical model (with a closed form solution) that provided a good fit to the data. This allowed us to infer the time scale of integration that accounts for the reliable component of eye movements. 
<br><br>
<small>
This work was <a href="http://www.ncbi.nlm.nih.gov/pubmed/22262911">published</a> in
Wang, HX, Freeman F, Merriam EP, Hasson U, Heeger DJ. Temporal eye movement strategies during naturalistic viewing. Journal of Vision, 12(1):16, 1-27, 2012. <a href="http://www.cns.nyu.edu/heegerlab/content/publications/Wang-JoV2012.pdf">reprint</a><br>
A preliminary version of this work was also presented at the <a href="http://www.journalofvision.org/content/10/7/528.short">2010 Vision Sciences Society</a> conference. 
</small>
</p>
<br>


<h4>
<a name="microsaccades" class="anchor" href="#microsaccades"><span class="octicon octicon-link"></span></a>Using microsaccades to infer neural computation</h4>
<p>  
Our eyes are never completely still. Even when we attempt to maintain stable fixation, we still make tiny eye movements called microsaccades. Microsaccades are modulated by changes in visual stimulation and cognitive state of the observer. Neurophysiologically, they also reflect neural activations near the foveal region of a continuous visuomotor map. Manipulating and measuring microsaccades therefore provides an opportunity to infer the neural computation and representation in the visuomotor map. I made a series of measurements to quantify how the rate and direction of microsaccades depend on visual stimulation during prolonged fixation, and used these measurements to infer and constrain the nature of spatiotemporal interactions of neural representations in the brain. 
<br><br>
<small>
This work was presented at the <a href="http://www.journalofvision.org/content/13/9/1338.short">2013 Vision Sciences Society</a> conference. 
</small>
</p>
<br>





      </section>
      <footer>
        <p><small>Hosted on GitHub Pages</small></p>
      </footer>
    </div>
    <script src="javascripts/scale.fix.js"></script>
    
  </body>
</html>