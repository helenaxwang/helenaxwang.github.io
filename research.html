<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <title>Helena Wang | Github Pages</title>

    <link rel="stylesheet" href="stylesheets/styles.css">
    <link rel="stylesheet" href="stylesheets/pygment_trac.css">
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
  </head>
  <body>
    <div class="wrapper">
      <header>
        <div class="profile-img">
          <img src="images/profile.jpg">
        </div>
        <h1>Helena Wang</h1>
        <p class="view">
          <a class="soc-icon" href="https://github.com/helenaxwang"><img src="images/ico_github.png"></a>
          <a class="soc-icon" href="https://www.linkedin.com/pub/helena-wang/3/3/61a"><img src="images/ico_linkedin.png"></a>
          <a class="soc-icon" href="https://twitter.com/helenaXwang"><img src="images/ico_twitter.png"></a>
        </p>
        <p class="view">
          <a href="index.html">About me</a><br>
          <a href="research.html">Research</a><br>
          <a href="publications.html">Publications</a>
        </p>

      </header>
      <section>
<h2>
<a name="research" class="anchor" href="#research"><span class="octicon octicon-link"></span></a>Research</h2>

<p>

Below is a top-level summary of some of the projects I have worked on, grouped more or less by topic. 

</p>

<h4>
<a name="motion" class="anchor" href="#motion"><span class="octicon octicon-link"></span></a>Encoding of visual motion in human visual cortex</h4>
<p>  
Multivariate pattern analysis is a commonly used analysis method to infer the perceptual and cognitive state of an observer from fMRI measurements. For example, in vision experiments, the response properties of a visual stimulus can be read out, or “decoded”, from the spatially distributed pattern of voxel responses in the brain. High decoding accuracy in a given visual area for a given visual property is commonly interpreted to indicate that neurons residing in that area encode (show selectivity for) that visual property. Our results provide evidence against this interpretation. We demonstrate that, in case of visual motion, fMRI-based motion decoding has little or no dependence on the underlying functional organization of motion selectivity. This work also investigates the spatial pattern of neural representations that are necessary for successful fMRI-based decoding. <br><br>

<small>
  This work was <a href="#">published</a> in Wang HX, Merriam EP, Freeman J, and Heeger DJ, 2014.<br>
  A <a href="http://www.abstractsonline.com/Plan/ViewAbstract.aspx?sKey=2b0ec77b-0ed1-44b2-808e-029538f32e80&cKey=3e78ef30-e0ec-46fc-9b0c-9b9010ca0269&mKey=%7b8D2A5BEC-4825-4CD6-9439-B42BB151D1CF%7d"> preliminary version </a> of this work was also presented at the 2013 <a href="http://www.sfn.org/Annual-Meeting/Neuroscience-2013">Society for Neuroscience conference</a> in San Diego. 
</small>
</p>
<br>

<h4>
<a name="mt" class="anchor" href="#mt"><span class="octicon octicon-link"></span></a>Encoding of visual motion in nonhuman primates</h4>
<p>  
Neurons in area MT of the macaque monkey encode visual motion. Some of these neurons encode the true motion directions of visual patterns, which is a non-trivial computation that requires the nonlinear integration of “component directions” corresponding to multiple local, oriented features within the pattern. In this work, I analyze a previously-studied population of ~800 MT cells to investigate how various response properties of these neurons contribute to their pattern direction selectivity. These statistical relationships help inform functional models underlying the mechanisms of motion integration by MT neurons. 
<br><br>
<small>This work was done in the laboratory of <a href="http://www.cns.nyu.edu/corefaculty/Movshon.php">Tony Movshon</a> and <a href="http://www.abstractsonline.com/Plan/ViewAbstract.aspx?sKey=f0b0bb1c-62f8-4e8a-baa2-438a5cde3e9e&cKey=30ca709d-49f3-4428-8c85-d3e1a796eee7&mKey=%7bE5D5C83F-CE2D-4D71-9DD6-FC7231E090FB%7d">presented</a> at the 2010 <a href="http://www.sfn.org/Annual-Meeting/Past-and-Future-Annual-Meetings/2010">Society for Neuroscience conference</a> in San Diego. </small>
</p>
<br>


<h4>
<a name="texture" class="anchor" href="#texture"><span class="octicon octicon-link"></span></a>Using perception of second-order visual stimuli to infer neural computation</h4>
<p>  
In pattern vision, “first-order” patterns are those with boundaries defined by changes in luminance (pixel intensity). The processing of first-order images is largely a linear computation known to take place in primary visual cortex (V1): the perceptual sensitivity to these images is linked systematically to the responses of V1 neurons. <a href="http://www.cns.nyu.edu/home/msl/#research">“Second-order”</a> patterns are those with boundaries defined by image statistics other than pixel intensity. The model commonly used to explain how neurons might compute, and how humans might perceive, this kind of boundaries involves a cascade of linear computations with intervening nonlinearities (e.g., filtering, rectification, normalization, filtering). It has also been proposed that each of these component computations can be thought of as a <a href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3273486/">“canonical” computation</a> that is ubiquitous in different areas of cortex and can be combined in various ways to perform different sensory and cognitive functions. In this study, I tested for the existence of and characterized the computation of normalization in second-order vision, by manipulating and measuring observers’ perceptual sensitivity to second-order visual stimuli. This allows us to extrapolate from the computational theories of first-order vision to make inferences about computations of higher-order visual processing and other areas of cortex beyond V1. 
<br><br>
<small>
This work was a collaboration with <a href = "http://www.cns.nyu.edu/home/msl/">Mike Landy</a>. <br>
It was <a href="http://www.ncbi.nlm.nih.gov/pubmed/22811987">published</a> in Wang HX, Heeger DJ, and Landy MS, 2012.&nbsp; 
A <a href="http://www.journalofvision.org/content/11/11/1173.abstract"> preliminary version </a> of this work was also presented at the 2011 <a href="http://www.visionsciences.org/">Vision Sciences Society</a> conference. 
</small>
</p>
<br>



<h4>
<a name="eyemovements" class="anchor" href="#eyemovements"><span class="octicon octicon-link"></span></a>Temporal reliability of exploratory eye movements</h4>
<p>  
How people move their eyes to complex, real-world visual stimuli likely involves a variety of uncontrolled sensory and cognitive factors. Yet surprisingly, to a certain class of dynamic, engaging stimuli, eye movements are highly reliable across observers and across repetitions. In this study, we exploited this reliabilty to understand the temporal dynamics of eye movements. Specifically, we asked how the reliable (shared) component of eye movements depends on the accumulation of visual information over time. So we systematically manipulated the temporal context of movie clips (by scrambling them), measured people’s eye movements, and developed a mathematical model (with a closed form solution) that provided an excellent fit to the behavior data. The model allowed us to infer the time scale of integration underlying eye movements for complex, dynamic stimuli. 
<br><br>
<small>
This work was <a href="http://www.ncbi.nlm.nih.gov/pubmed/22262911">published</a> in
Wang, HX, Freeman J, Merriam EP, Hasson U and Heeger DJ, 2012. <br>
A <a href="http://www.journalofvision.org/content/10/7/528.short">preliminary version</a> of this work was also presented at the 2010 <a href="http://www.visionsciences.org/">Vision Sciences Society</a> conference. 
</small>
</p>
<br>


<h4>
<a name="microsaccades" class="anchor" href="#microsaccades"><span class="octicon octicon-link"></span></a>Using microsaccades to infer neural computation</h4>
<p>  
Our eyes are never completely still. Even when we attempt to maintain stable fixation, we still make tiny eye movements called microsaccades. Microsaccades are modulated by changes in visual stimulation and cognitive state of the observer. Neurophysiologically, they also reflect neural activations near the foveal region of a continuous visuomotor map (such as in a part of the brain called the superior colliculus). Manipulating and measuring microsaccades behaviorally therefore provides an opportunity to infer the neural computation and representation in visuomotor maps in the brain. I made a series of measurements to quantify how the rate and direction of microsaccades depend on visual stimulation during prolonged fixation, and used these measurements to infer and constrain the nature of spatiotemporal interactions of neural representations in the brain. 
<br><br>
<small>
This <a href="http://www.journalofvision.org/content/13/9/1338.short">work</a> was presented at the 2013 <a href="http://www.visionsciences.org/">Vision Sciences Society</a> conference. 
</small>
</p>
<br>

      </section>
      <footer>
        <p><small>Hosted on GitHub Pages</small></p>
      </footer>
    </div>
    
  <script src="javascripts/scale.fix.js"></script>
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-55143546-2', 'auto');
    ga('send', 'pageview');
  </script>
    
  </body>
</html>